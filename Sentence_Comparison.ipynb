{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install textstat\n",
        "!pip install bert_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXyG-uSeHgcz",
        "outputId": "15be922e-117b-4d39-f5eb-e027a039db5e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textstat\n",
            "  Downloading textstat-0.7.5-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pyphen (from textstat)\n",
            "  Downloading pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting cmudict (from textstat)\n",
            "  Downloading cmudict-1.0.32-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from textstat) (75.1.0)\n",
            "Requirement already satisfied: importlib-metadata>=5 in /usr/local/lib/python3.11/dist-packages (from cmudict->textstat) (8.6.1)\n",
            "Requirement already satisfied: importlib-resources>=5 in /usr/local/lib/python3.11/dist-packages (from cmudict->textstat) (6.5.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=5->cmudict->textstat) (3.21.0)\n",
            "Downloading textstat-0.7.5-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.3/105.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cmudict-1.0.32-py3-none-any.whl (939 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m939.4/939.4 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen, cmudict, textstat\n",
            "Successfully installed cmudict-1.0.32 pyphen-0.17.2 textstat-0.7.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVDoSxkt8yq-",
        "outputId": "5c7dd261-46d3-4ba9-bc2f-d3681090a478"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique Tokens: {'working', 'efficient', ',', 'policies', 'At', 'To', 'Physical', 'Governments', 'make', 'same', 'the', 'with', 'needed', 'digital', 'networks', \"it's\", 'automation', 'environment', 'businesses', 'on', 'worldwide', 'build', 'create', 'work', 'be', 'should', 'standards', 'global', 'that', 'infrastructure', 'support', 'put', 'time', 'in', 'industry', 'focus', '.', 'along', 'important', 'of', 'to', 'set', 'sustainability', 'rewards', 'use', 'invest', 'technologies', 'Internet', 'encourage', 'and', 'place', 'logistics'}\n",
            "Metrics: {'WC': 70, 'TTR': 0.7428571428571429, 'ADD': 3.1044776119402986, 'FKG': 13.7}\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "import json\n",
        "import spacy\n",
        "import textstat\n",
        "\n",
        "def unique_tokens(text, tokenizer=nltk.TweetTokenizer().tokenize):\n",
        "    \"\"\"\n",
        "    Tokenizes the input text and returns the set of unique tokens.\n",
        "    \"\"\"\n",
        "    return set(tokenizer(text))\n",
        "\n",
        "def calculate_metrics(text):\n",
        "    \"\"\"\n",
        "    Calculates WC (Word Count), TTR (Type-Token Ratio),\n",
        "    ADD (Average Dependency Distance), and FKG (Flesch-Kincaid Grade Level).\n",
        "    \"\"\"\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    word_count = len(tokens)\n",
        "    unique_word_count = len(set(token.lower() for token in tokens))\n",
        "    ttr = unique_word_count / word_count if word_count > 0 else 0\n",
        "\n",
        "    # Dependency parsing\n",
        "    doc = nlp(text)\n",
        "    dependencies = [abs(token.i - token.head.i) for token in doc if token.head.i != token.i]\n",
        "    add = sum(dependencies) / len(dependencies) if dependencies else 0\n",
        "\n",
        "    # Readability score\n",
        "    fkg = textstat.flesch_kincaid_grade(text)\n",
        "\n",
        "    return {\"WC\": word_count, \"TTR\": ttr, \"ADD\": add, \"FKG\": fkg}\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    text = \"To make the Physical Internet work worldwide, it's important to set global standards and build the needed infrastructure. Governments should support the use of digital and automation technologies, working with the logistics industry to create efficient networks. At the same time, policies that focus on the environment and sustainability should be put in place, along with rewards to encourage businesses to invest.\"\n",
        "    print(\"Unique Tokens:\", unique_tokens(text))\n",
        "    print(\"Metrics:\", calculate_metrics(text))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cosine Similarity\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "# Text\n",
        "sentence1 = \"While the ALICE and Japan roadmaps lay a solid foundation for the Physical Internet (PI), several research gaps remain. Key areas include the development of universal standards for interoperability, secure data-sharing mechanisms, and more robust sustainability metrics. There is also limited research on decentralized logistics models, which could improve resilience. Emerging technologies like IoT, AI, and blockchain need further exploration for integration into the PI framework. Additionally, workforce impacts, including job displacement and retraining, are insufficiently addressed. Lastly, economic models and funding mechanisms for PI adoption require more in-depth study.\"\n",
        "sentence2 = \"The areas lacking in research focusing on the physical internet, compared to the ALICE and Japan roadmaps, include insufficient exploration of container development, hub inventory management, and the integration of Internet of Things (IoT) technologies in risk management within prefabricated construction. Additionally, there is a need for deeper analysis of protocols and methods for resilience and efficiency in transport concepts, as well as a more comprehensive understanding of the functionalities and attributes of road-based physical internet systems.\"\n",
        "\n",
        "embeddings1 = model.encode(sentence1, convert_to_tensor=True)\n",
        "embeddings2 = model.encode(sentence2, convert_to_tensor=True)\n",
        "\n",
        "cosine_score = util.pytorch_cos_sim(embeddings1, embeddings2)[0][0]\n",
        "\n",
        "print(f\"Similarity between sentence 1 and sentence 2: {cosine_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "899b8TgjF5Vc",
        "outputId": "4064a51c-3840-4148-fa17-365afc34d4ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between sentence 1 and sentence 2: 0.6708195209503174\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BERTScore & SentenceBERT\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from bert_score import score\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Text\n",
        "sentence1 = \"The Physical Internet (PI) is a global system to make logistics work like the digital internet. However, more research is needed in areas like standardization, sustainability, security, and using drones and autonomous vehicles. It's also important to study how it will affect society, the economy, and what rules are needed to make it work.\"\n",
        "sentence2 = \"The areas of research that are missing in the context of the Physical Internet include a comprehensive exploration of the strengths, risks, challenges, and potential barriers to implementation. Additionally, there is a lack of studies focusing on the integration of hyperconnected logistics networks, the operationalization of the Physical Internet using frameworks similar to the digital internet, and the development of robust business models to support its adoption. Further research is also needed on the stochasticity and resilience of hub location and network design in large-scale optimization problems.\"\n",
        "\n",
        "P, R, F1 = score([sentence1], [sentence2], lang='en')\n",
        "\n",
        "print(f\"BERTScore Precision: {P[0]:.4f}\")\n",
        "print(f\"BERTScore Recall: {R[0]:.4f}\")\n",
        "print(f\"BERTScore F1: {F1[0]:.4f}\")\n",
        "\n",
        "model_sentenceBERT = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "embedding1_sentenceBERT = model_sentenceBERT.encode(sentence1)\n",
        "embedding2_sentenceBERT = model_sentenceBERT.encode(sentence2)\n",
        "\n",
        "cosine_sim_sentenceBERT = cosine_similarity([embedding1_sentenceBERT], [embedding2_sentenceBERT])[0][0]\n",
        "\n",
        "print(f\"Cosine Similarity (SentenceBERT): {cosine_sim_sentenceBERT:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bx42Yh9txlj",
        "outputId": "ca916264-4a9b-41d4-dd90-c6012d646aec"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERTScore Precision: 0.8842\n",
            "BERTScore Recall: 0.8681\n",
            "BERTScore F1: 0.8761\n",
            "Cosine Similarity (SentenceBERT): 0.5252\n"
          ]
        }
      ]
    }
  ]
}