{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import os\n",
    "import pandas as pd\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import Document \n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from neo4j import GraphDatabase\n",
    "from yfiles_jupyter_graphs import GraphWidget\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = ''\n",
    "os.environ[\"NEO4J_USERNAME\"] = ''\n",
    "os.environ[\"NEO4J_URI\"] = ''\n",
    "os.environ[\"NEO4J_PASSWORD\"] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "csv_file = 'PATH'  \n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "translated_texts = df['Text'].dropna().tolist()  \n",
    "raw_documents = [Document(page_content=text) for text in translated_texts]\n",
    "\n",
    "\n",
    "text_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=24)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "\n",
    "for doc in documents[:5]:  \n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding\n",
    "from tqdm import tqdm\n",
    "\n",
    "llm=ChatOpenAI(temperature=0, model_name=\"MODELNAME\") #ex) gpt-4o\n",
    "llm_transformer = LLMGraphTransformer(llm=llm)\n",
    "\n",
    "graph_documents = []\n",
    "for doc in tqdm(documents, desc=\"Transforming documents to graph format\"):\n",
    "    graph_doc = llm_transformer.convert_to_graph_documents([doc])\n",
    "    graph_documents.extend(graph_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import into Neo4j\n",
    "graph = Neo4jGraph(\n",
    "    url=os.environ[\"NEO4J_URI\"],\n",
    "    username=os.environ[\"NEO4J_USERNAME\"],\n",
    "    password=os.environ[\"NEO4J_PASSWORD\"],\n",
    "    database=\"DATABASE\"  # Enter the name of the database you wish to specify here\n",
    ")\n",
    "for graph_doc in tqdm(graph_documents, desc=\"Adding graph documents to Neo4j\"):\n",
    "    graph.add_graph_documents(\n",
    "        [graph_doc],\n",
    "        baseEntityLabel=True,\n",
    "        include_source=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GraphRAG Settings\n",
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough\n",
    "from langchain.vectorstores.neo4j_vector import Neo4jVector\n",
    "from langchain.graphs import Neo4jGraph\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "graph = Neo4jGraph()\n",
    "graph.query(\n",
    "    \"CREATE FULLTEXT INDEX entity IF NOT EXISTS FOR (e:__Entity__) ON EACH [e.id]\"\n",
    ")\n",
    "\n",
    "class Entities(BaseModel):\n",
    "    names: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"All the person, organization, or business entities that appear in the text\",\n",
    "    )\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Entities)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are extracting organization and person entities from the text.\"),\n",
    "        (\"human\", \"Extract information from the following input in JSON format:\\n{question}\\n\\n{format_instructions}\"),\n",
    "    ]\n",
    ").partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\")\n",
    "\n",
    "entity_chain = prompt | llm | parser\n",
    "\n",
    "def structured_retriever(question: str) -> str:\n",
    "    result = \"\"\n",
    "    entities = entity_chain.invoke({\"question\": question})\n",
    "    for entity in entities.names:\n",
    "        response = graph.query(\n",
    "            \"\"\"CALL db.index.fulltext.queryNodes('entity', $query, {limit:2})\n",
    "            YIELD node, score\n",
    "            CALL {\n",
    "              WITH node\n",
    "              MATCH (node)-[r:!MENTIONS]->(neighbor)\n",
    "              RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\n",
    "              UNION ALL\n",
    "              WITH node\n",
    "              MATCH (node)<-[r:!MENTIONS]-(neighbor)\n",
    "              RETURN neighbor.id + ' - ' + type(r) + ' -> ' + node.id AS output\n",
    "            }\n",
    "            RETURN output LIMIT 50\n",
    "            \"\"\",\n",
    "            {\"query\": entity},\n",
    "        )\n",
    "        result += \"\\n\".join([el['output'] for el in response])\n",
    "    return result\n",
    "\n",
    "def retriever(question: str, mode: str = \"hybrid\"):\n",
    "    print(f\"Search query: {question} | Mode: {mode}\")\n",
    "\n",
    "    if mode == \"gpt_only\":\n",
    "        return \"\"  \n",
    "\n",
    "    structured_data = \"\"\n",
    "    unstructured_data = []\n",
    "\n",
    "    if mode in [\"local\", \"hybrid\"]:\n",
    "        structured_data = structured_retriever(question)\n",
    "\n",
    "    if mode in [\"global\", \"hybrid\"]:\n",
    "        unstructured_data = [\n",
    "            el.page_content for el in vector_index.similarity_search(question)\n",
    "        ]\n",
    "\n",
    "    final_data = f\"\"\"Structured data:\n",
    "{structured_data}\n",
    "Unstructured data:\n",
    "{\"#Document \".join(unstructured_data)}\n",
    "\"\"\"\n",
    "    print(final_data)\n",
    "    return final_data\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Use natural language and be concise.\n",
    "Answer:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def qa_chain(question: str, mode: str = \"hybrid\"):\n",
    "    if mode == \"gpt_only\":\n",
    "        return llm.invoke(question)\n",
    "\n",
    "    chain = (\n",
    "        RunnableParallel(\n",
    "            {\n",
    "                \"context\": lambda q: retriever(q, mode=mode),\n",
    "                \"question\": RunnablePassthrough(),\n",
    "            }\n",
    "        )\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#response generation\n",
    "if __name__ == \"__main__\":\n",
    "    question = \"PROMPT\" #prompt\n",
    "\n",
    "    print(\"=== GPT ONLY ===\")\n",
    "    print(qa_chain(question, mode=\"gpt_only\"))\n",
    "\n",
    "    print(\"\\n=== LOCAL SEARCH ===\")\n",
    "    print(qa_chain(question, mode=\"local\"))\n",
    "\n",
    "    print(\"\\n=== GLOBAL SEARCH ===\")\n",
    "    print(qa_chain(question, mode=\"global\"))\n",
    "\n",
    "    print(\"\\n=== HYBRID SEARCH ===\")\n",
    "    print(qa_chain(question, mode=\"hybrid\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
